import org.apache.tools.ant.util.LazyFileOutputStream
import org.apache.tools.ant.util.OutputStreamFunneler

import java.nio.file.Files
import java.nio.file.Paths
import java.util.regex.Pattern

import static java.nio.file.StandardCopyOption.REPLACE_EXISTING

buildscript {
    repositories {
        mavenCentral()
        maven {
            url "https://plugins.gradle.org/m2/"
        }
    }
}

plugins {
    id "java"
    id "idea"
    id "com.adarshr.test-logger" version "3.2.0"
    id "de.undercouch.download" version "5.3.1"
    id "com.gradleup.shadow" version "8.3.2"
}

// ===========================================================================
// plugin info
// ===========================================================================
group                      'co.elastic.logstash.plugins.filter.elasticintegration' // must match the package of the main plugin class
version                    "${file("VERSION").text.trim()}" // read from required VERSION file
description                = "Elastic Integration filter"
// ===========================================================================

java {
    sourceCompatibility = 21
    targetCompatibility = 21
}
tasks.withType(JavaCompile) {
    options.encoding = 'UTF-8'
}

repositories {
    mavenCentral()
}

configurations {
    logstashCore { canBeResolved = false; canBeConsumed = false }
    elasticsearchMinimalCore { canBeConsumed = false }
    elasticsearchClient { canBeConsumed = false }

    geolite2 { canBeConsumed = false }

    mockitoAgent { canBeConsumed = false }

    implementation.extendsFrom(logstashCore, elasticsearchMinimalCore, elasticsearchClient)
}

wrapper {
    gradleVersion = '8.7'
    distributionUrl = distributionUrl.replace("bin", "all")
}

shadowJar {
    configurations = [project.configurations.elasticsearchMinimalCore,
                      project.configurations.elasticsearchClient]

    zip64 true
}

dependencies {
    elasticsearchClient('co.elastic.clients:elasticsearch-java') {
        // brings latest version available for 8.x branch
        version {
            strictly('[8.0, 9.0[')
        }
    }
    elasticsearchMinimalCore fileTree(dir: { importMinimalElasticsearch.jars }, include: ["*.jar"])

    // Logstash core and its known-provided and required dependencies
    // must NEVER be included in the gem-vendored jars
    logstashCore files(objects.fileCollection(), {
        from requiredLogstashCoreJar("logstash-core")
        from requiredLogstashCoreJar("log4j-api")
        from requiredLogstashVendorJar("jruby/lib", "jruby")
    })

    // Logstash core-provided dependencies that are also used directly
    // by this plugin's implementation and SHOULD be gem-vendored
    // once they can be safely shaded
    logstashCore files(objects.fileCollection(), {
        from requiredLogstashCoreJar("jackson-core")
        from requiredLogstashCoreJar("jackson-databind")
        from requiredLogstashCoreJar("jackson-annotations")
        from requiredLogstashCoreJar("guava", "jre")
    })

    mockitoAgent('org.mockito:mockito-core:5.14.1') {
        transitive = false
    }
    testImplementation 'org.junit.jupiter:junit-jupiter-api:5.1.0'
    testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.1.0'
    testImplementation 'org.mockito:mockito-junit-jupiter:5.14.1'
    testImplementation 'org.hamcrest:hamcrest-library:2.2'
    testImplementation "com.github.seregamorph:hamcrest-more-matchers:0.1"
    testImplementation 'org.wiremock:wiremock:3.12.0'
    testRuntimeOnly requiredLogstashCoreJar("log4j-core")

    geolite2('org.elasticsearch:geolite2-databases:20191119') {
        transitive = false
    }
}

/**
 * @param packageNameSpec e.g., "package-name" or "nesting/path/package-name"
 * @return a {@code Closure<Boolean>} determines if the provided {@code FileTreeElement}
 *         is a package matching the {@code packageNameSpec} or a nesting that _could_ include it.
 */
def jarPackageNamed(String packageNameSpec, String packageFlavorSpec = null) {
    def packageNamePath = Paths.get(packageNameSpec)
    def packageName = packageNamePath.fileName.toString()
    def packageNesting = packageNamePath.parent
    def packagePattern = namedPackageWithAnyVersionPattern(packageName, packageFlavorSpec, "jar")

    return { FileTreeElement element ->
        if (packageNesting != null && packageNesting.startsWith(element.relativePath.toString())) { return true }
        if (packageNesting == null && element.relativePath.toString() != element.name) { return false }

        return element.name.matches(packagePattern)
    }
}

def namedPackageWithAnyVersionPattern(String packageName, String packageFlavorSpec = null, String suffix = null) {
    def escapedPackagePart = Pattern.quote(packageName)
    def optionalAnyVersionPart = "(?:-\\d+(?:\\.\\d+)*(?i:-SNAPSHOT(?:-[0-9A-F]+)?)?)?"
    def escapedFlavorPart = packageFlavorSpec == null ? "" : "(?:-${Pattern.quote(packageFlavorSpec)})"
    def escapedSuffixPart = suffix == null ? "" : Pattern.quote(".${suffix}")
    return Pattern.compile("${escapedPackagePart}${optionalAnyVersionPart}${escapedFlavorPart}${escapedSuffixPart}");
}

def envOrPropertyValue(String name) {
    return System.getenv(name) ? System.getenv(name) : project.findProperty(name)
}

def requiredLogstashCoreJar(jarSpec, flavorSpec = null) {
    _requiredLogstashJar("logstash-core/lib/jars", jarSpec, flavorSpec)
}

def requiredLogstashVendorJar(path, jarSpec) {
    _requiredLogstashJar("vendor/${path}", jarSpec)
}

def _requiredLogstashJar(pathPrefix, jarSpec, flavorSpec = null) {
    def logstashPath = envOrPropertyValue("LOGSTASH_PATH")
    assert logstashPath != null : "The property LOGSTASH_PATH must be defined, and be path to logstash directory"
    file("${logstashPath}").with { logstashDir ->
        assert logstashDir.exists() : "LOGSTASH_PATH does not exist: ${logstashDir}"
        assert logstashDir.directory : "LOGSTASH_PATH is not a directory: ${logstashDir}"
        file("${logstashDir.path}/${pathPrefix}/").with { jarSourceDir ->
            fileTree(jarSourceDir) { include jarPackageNamed(jarSpec, flavorSpec) }.tap { jarSource ->
                assert !jarSource.empty : "LOGSTASH_PATH's ${pathPrefix} does not provide any ${jarSpec} jars: ${jarSourceDir}"
                assert jarSource.files.size() == 1 : "LOGSTASH_PATH's ${pathPrefix} provides multiple ${jarSpec} jars: ${jarSource.files}"
            }
        }
    }
}

static OutputStreamFunneler outputStreamFunneler(File logFile) {
    logFile.parentFile.mkdirs()
    logFile.delete()
    logFile.createNewFile()

    return new OutputStreamFunneler(new LazyFileOutputStream(logFile))
}

// https://docs.github.com/en/repositories/working-with-files/using-files/downloading-source-code-archives#source-code-archive-urls
String githubArchivePath(repo, treeish="main", archiveFormat="zip") {
    def pathFragment = {
        switch(treeish) {
            case ~'^refs/(?:heads|tags)/'       : return "${treeish}" // unambiguous ref
            case ~'^[0-9a-f]{8,40}$'            : return "${treeish}" // possibly-abbreviated commit sha
            case ~'^v[0-9]+[.][0-9]+[.][0-9]+$' : return "refs/tags/${treeish}"  // guess: version tag
            default                             : return "refs/heads/${treeish}" // guess: branch name
        }
    }()

    "https://github.com/${repo}/archive/${pathFragment}.${archiveFormat}"
}

task downloadElasticsearchSourceZip(type: Download) {
    def esTreeish = envOrPropertyValue("ELASTICSEARCH_TREEISH")
    inputs.property("ELASTICSEARCH_TREEISH", esTreeish)

    if (!esTreeish) {
        throw new StopActionException("""
            The property ELASTICSEARCH_TREEISH must be defined, and be a tree-ish (branch, commit hash, or tag)
            available on the github repository for elastic/elasticsearch (e.g., `v8.7.0`, `8.7`, `main`,
            `ff1caebad1dea`, `refs/heads/branch-name`, `refs/tags/tag-name`)
        """)
    }

    def esRepo = envOrPropertyValue("ELASTICSEARCH_REPO") ?: "elastic/elasticsearch"
    inputs.property("ELASTICSEARCH_REPO", esRepo)

    src githubArchivePath(esRepo, esTreeish, "zip")
    dest "${buildDir}/elasticsearch-source.zip"
    onlyIfModified true
    useETag "all"
}

task unzipDownloadedElasticsearchSourceZip(dependsOn: downloadElasticsearchSourceZip, type: Copy) {
    description "extracts Elasticsearch source from a downloaded zip file"

    ext.location = "${buildDir}/elasticsearch-source/"

    from zipTree(downloadElasticsearchSourceZip.dest)
    into ext.location
    eachFile {
        // strip top-level directory
        path = path.replaceFirst(/^.+?\//, "")
    }
}

task buildElasticsearchLocalDistro(dependsOn: unzipDownloadedElasticsearchSourceZip, type: Exec) {
    description "builds Elasticsearch localDistro"

    def logFile = project.file("${buildDir}/elasticsearch-build.log")
    doFirst {
        def funneler = outputStreamFunneler(logFile)
        standardOutput = funneler.funnelInstance
        errorOutput = funneler.funnelInstance
    }

    def esSource = "${unzipDownloadedElasticsearchSourceZip.outputs.files.singleFile}"
    def esBuildDir = "${esSource}/build"

    outputs.dir esBuildDir

    ext.buildRoot = esBuildDir
    ext.localDistroParent = "${esBuildDir}/distribution/local"
    ext.localDistroResult = provider({
        project.fileTree(dir: ext.localDistroParent, include: "*/lib/elasticsearch-core-*.jar").with {
            it && !it.isEmpty() ? it.singleFile.parentFile.parentFile : null
        }
    })
    ext.lib = localDistroResult.map { "${it}/lib" }
    ext.module = { moduleName -> localDistroResult.map { "${it}/modules/${moduleName}"} }

    workingDir esSource
    commandLine "./gradlew", "--stacktrace", "localDistro"

    ignoreExitValue true // handled in doLast
    doLast {
        def exitValue = executionResult.get().exitValue
        if (exitValue != 0) {
            if (logFile.exists()) {
                println "\n===== Elasticsearch Build Log ====="
                println logFile.text
                println "===== End of Build Log =====\n"
            } else {
                "Elasticsearch build failed and ${logFile.path} log does not exist"
            }
            throw new GradleException("Elasticsearch build failed, see the logs for details.")
        }
        assert ext.localDistroResult.isPresent() : "Elasticsearch did not produce exactly one localdistro build"
    }
}

task buildElasticsearchLogstashBridge(type: Exec) {
    description "builds logstash-bridge lib module"

    dependsOn unzipDownloadedElasticsearchSourceZip
    dependsOn buildElasticsearchLocalDistro // mustRunAfter?

    def logFile = project.file("${buildDir}/logstash-bridge-build.log")
    doFirst {
        def funneler = outputStreamFunneler(logFile)
        standardOutput = funneler.funnelInstance
        errorOutput = funneler.funnelInstance
    }

    def esSource = "${unzipDownloadedElasticsearchSourceZip.outputs.files.singleFile}"
    def esBuildDir = "${esSource}/build"

    inputs.dir "${esSource}/libs/logstash-bridge"

    outputs.dir("${esSource}/libs/logstash-bridge/build/distributions")

    ext.buildRoot = esBuildDir
    workingDir esSource
    commandLine "./gradlew", ":lib:logstash-bridge:build"

    ignoreExitValue true // handled in doLast
    doLast {
        def exitValue = executionResult.get().exitValue
        if (exitValue != 0) {
            if (logFile.exists()) {
                println "\n===== Elasticsearch logstash-bridge build Log ====="
                println logFile.text
                println "===== End of Elasticsearch logstash-bridge build Log =====\n"
            } else {
                "Elasticsearch logstash-bridge build failed and ${logFile.path} log does not exist"
            }
            throw new GradleException("Elasticsearch logstash-bridge build failed, see the logs for details.")
        }
    }
}

def ingestGeoipPluginShadeNamespace = "org.elasticsearch.ingest.geoip.shaded"

/**
 * The StableBridge exposes GeoIP plugin internals, so it needs to relocate references to
 * its bundled dependencies to match the shaded locations in our import of that plugin.
 */
task shadeElasticsearchStableBridge(type: com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar) {
    description "Shades Maxmind dependencies"

    dependsOn buildElasticsearchLogstashBridge

    from(buildElasticsearchLogstashBridge)

    archiveFileName = "logstash-stable-bridge-shaded.jar"
    destinationDirectory = file("${buildDir}/shaded")

    relocate('com.fasterxml.jackson', "${ingestGeoipPluginShadeNamespace}.com.fasterxml.jackson")
    relocate('com.maxmind',           "${ingestGeoipPluginShadeNamespace}.com.maxmind")

    mergeServiceFiles()
}

task shadeElasticsearchIngestGeoIpModule(type: com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar) {
    description "Shades embedded dependencies of the Elasticsearch Ingest GeoIP module"

    dependsOn buildElasticsearchLocalDistro

    from(buildElasticsearchLocalDistro.module("ingest-geoip").orElse(objects.fileCollection()))

    archiveFileName = 'ingest-geoip-shaded.jar'
    destinationDirectory = file("${buildDir}/shaded")

    relocate('com.fasterxml.jackson', "${ingestGeoipPluginShadeNamespace}.com.fasterxml.jackson")
    relocate('com.maxmind',           "${ingestGeoipPluginShadeNamespace}.com.maxmind")

    mergeServiceFiles()

    exclude '**/module-info.class'
}

def ingestGrokPluginShadeNamespace = "org.elasticsearch.grok.shaded"

task shadeElasticsearchGrokImplementation(type: com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar) {
    description "Shades embedded dependencies of the Elasticsearch Grok implementation"

    dependsOn buildElasticsearchLocalDistro

    from(buildElasticsearchLocalDistro.lib.orElse(objects.fileCollection())) {
        include jarPackageNamed("elasticsearch-grok")
        include jarPackageNamed("joni")
        include jarPackageNamed("jcodings")
    }

    archiveFileName = "elasticsearch-grok-uber.jar"
    destinationDirectory = file("${buildDir}/shaded")

    mergeServiceFiles()
    relocate('org.joni', "${ingestGrokPluginShadeNamespace}.org.joni")
    relocate('org.jcodings', "${ingestGrokPluginShadeNamespace}.org.jcodings")

    exclude '**/module-info.class'
}

/**
 * The x-pack redact plugin reaches into the grok plugin's implementation, so
 * they both need to point to the same relocated shaded components.
 */
task shadeElasticsearchRedactPlugin(type: com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar) {
    description "Shades Elasticsearch Redact plugin to reference Grok's shaded dependencies"
    dependsOn buildElasticsearchLocalDistro

    from(buildElasticsearchLocalDistro.module("x-pack-redact").orElse(objects.fileCollection())) {
        include jarPackageNamed("x-pack-redact")
    }
    archiveFileName = "elasticsearch-x-pack-redact-shaded.jar"
    destinationDirectory = file("${buildDir}/shaded")

    // relocate elasticsearch-grok's dependencies to match
    relocate('org.joni', "${ingestGrokPluginShadeNamespace}.org.joni")
    relocate('org.jcodings', "${ingestGrokPluginShadeNamespace}.org.jcodings")

    exclude '**/module-info.class'
}

task importMinimalElasticsearch() {
    description "Imports minimal portions of Elasticsearch localDistro"

    dependsOn buildElasticsearchLocalDistro
    dependsOn shadeElasticsearchStableBridge
    dependsOn shadeElasticsearchIngestGeoIpModule
    dependsOn shadeElasticsearchGrokImplementation
    dependsOn shadeElasticsearchRedactPlugin

    ext.jars = "${buildDir}/elasticsearch-minimal-jars"

    inputs.dir buildElasticsearchLocalDistro.buildRoot
    outputs.dir ext.jars

    doLast {
        delete(project.file(ext.jars))

        copy {
            from(buildElasticsearchLocalDistro.lib) {
                include jarPackageNamed("elasticsearch")
                include jarPackageNamed("elasticsearch-core")
                include jarPackageNamed("elasticsearch-logging")
                include jarPackageNamed("elasticsearch-x-content")
                include jarPackageNamed("elasticsearch-geo")
                include jarPackageNamed("lucene-core")
                include jarPackageNamed("lucene-analysis-common")
                include jarPackageNamed("hppc")
            }
            from(shadeElasticsearchStableBridge.outputs.files.singleFile)
            from(shadeElasticsearchGrokImplementation)
            from(buildElasticsearchLocalDistro.module("x-pack-core"))

            from(buildElasticsearchLocalDistro.module("ingest-common")) {
                include jarPackageNamed("ingest-common")
                include jarPackageNamed("elasticsearch-dissect")
            }

            from(buildElasticsearchLocalDistro.module("ingest-user-agent")) {
                include jarPackageNamed("ingest-user-agent")
            }

            from(shadeElasticsearchIngestGeoIpModule)
            from(shadeElasticsearchRedactPlugin)

            from(buildElasticsearchLocalDistro.module("lang-mustache")) {
                include jarPackageNamed("lang-mustache")
                include jarPackageNamed("compiler")
            }

            from(buildElasticsearchLocalDistro.module("lang-painless")) {
                include jarPackageNamed("lang-painless")
                include jarPackageNamed("antlr4-runtime")
                include jarPackageNamed("asm-util")
                include jarPackageNamed("asm-commons")
                include jarPackageNamed("asm-tree")
                include jarPackageNamed("asm")
                include jarPackageNamed("spi/elasticsearch-scripting-painless-spi")
            }

            from(buildElasticsearchLocalDistro.module("wildcard")) {
                include jarPackageNamed("x-pack-wildcard")
            }
            from(buildElasticsearchLocalDistro.module("constant-keyword")) {
                include jarPackageNamed("x-pack-constant-keyword")
            }
            from(buildElasticsearchLocalDistro.module("spatial")) {
                include jarPackageNamed("spatial")
            }

            into ext.jars

            includeEmptyDirs(false)
            eachFile { path = name } // flatten
            eachFile { logger.info "copying ${it} to ${ext.jars}" }
        }
    }
}

/**
 * Verifies that imported Elasticsearch JARs have all required dependencies available.
 *
 * Scans class files for references and checks they can be resolved from:
 * - Classes within the imported JAR set
 * - JDK standard library (java.*, javax.*, etc.)
 * - Packages provided by Logstash at runtime (log4j, jackson, guava, etc.)
 *
 * Missing classes not in the above categories will fail the build to prevent
 * ClassNotFoundException at runtime. Optional/unused packages can be added to
 * the exclusion list if they are known to be safe (dead code paths, reflection-loaded, etc.)
 */
task verifyImportedJars() {
    description "Verifies imported JARs for dependency safety"

    dependsOn importMinimalElasticsearch

    def jarsDir = importMinimalElasticsearch.jars
    def reportDir = "${buildDir}/verify-dependencies"

    inputs.dir jarsDir
    outputs.dir reportDir

    // Should fail the build (default: true)
    ext.failOnMissingDeps = project.findProperty('jdeps.failOnMissingDeps') != 'false'

    // Logstash provided packages
    ext.providedPackages = [
        'org.apache.logging.log4j',
        'org.apache.log4j',
        'org.slf4j',
        'org.yaml.snakeyaml',
        'com.fasterxml.jackson.core',
        'com.fasterxml.jackson.databind',
        'com.fasterxml.jackson.annotation',
        'com.fasterxml.jackson.dataformat',
        'com.google.common'
    ]

    // TODO: following packages fail the build.
    // And we don't know if they (except coming through logstash-bridge) are truly dependencies and need to be added.
    // Let's address them later with
    ext.optionalPackages = [
        // logstash-bridge includes
        'org.elasticsearch.xcontent',
        'co.elastic.logging', // 9.x
        'org.elasticsearch.logging', // 8.x

        // Legacy logging fallbacks in commons-logging
        'org.apache.avalon.framework',
        'org.apache.log',

        // Optional compression/TLS in HTTP clients
        'org.brotli',
        'org.conscrypt',

        // ES internal packages not used in Logstash context
        'org.elasticsearch.cli',                    // CLI tools
        'org.elasticsearch.entitlement',            // Entitlement system
        'org.elasticsearch.nativeaccess',           // Native access
        'org.elasticsearch.plugin.analysis',        // Analysis plugins
        'org.elasticsearch.plugin.settings',        // Plugin settings
        'org.elasticsearch.plugin.Inject',          // Plugin injection

        // Lucene modules not needed for ingest processing
        'org.apache.lucene.internal.vectorization', // Vectorization
        'org.apache.lucene.backward_codecs',        // Old index format support
        'org.apache.lucene.queries.intervals',      // Interval queries
        'org.apache.lucene.queries.spans',          // Span queries
        'org.apache.lucene.queryparser',            // Query parsing
        'org.apache.lucene.sandbox',                // Sandbox features
        'org.apache.lucene.search.grouping',        // Grouping
        'org.apache.lucene.search.highlight',       // Highlighting
        'org.apache.lucene.search.join',            // Join queries
        'org.apache.lucene.search.spell',           // Spell checking
        'org.apache.lucene.search.suggest',         // Suggestions
        'org.apache.lucene.search.uhighlight',      // Unified highlighter
        'org.apache.lucene.search.vectorhighlight', // Vector highlighter
        'org.apache.lucene.misc',                   // Misc utilities
        'org.apache.lucene.index.memory',           // Memory index
        'org.apache.lucene.spatial3d',              // Spatial 3D
        'org.apache.lucene.store',                  // Store
        'org.apache.lucene.queries.function',       // Function queries

        // ES features not used in Logstash ingest
        'org.elasticsearch.exponentialhistogram',   // Histogram aggregations
        'org.elasticsearch.simdvec',                // SIMD vectors
        'org.elasticsearch.tdigest',                // T-Digest aggregations
        'org.elasticsearch.h3',                     // H3 geo
        'org.elasticsearch.legacygeo',              // Legacy geo
        'org.elasticsearch.lz4',                    // LZ4 compression lib
        'org.HdrHistogram',                         // HDR Histogram
        'net.jpountz.lz4',                          // LZ4 Java
        'net.jpountz.util',
        'joptsimple',                               // CLI option parsing

        // ASM tree analysis (only needed for bytecode verification, not runtime)
        'org.objectweb.asm.tree.analysis',
    ]

    doLast {
        file(reportDir).mkdirs()

        def jarFiles = fileTree(jarsDir).matching { include "*.jar" }.files
        if (jarFiles.isEmpty()) {
            throw new GradleException("No JAR files found in ${jarsDir}")
        }

        logger.lifecycle("Verifying ${jarFiles.size()} JAR files")
        logger.lifecycle("JARs directory: ${jarsDir}")

        // Collect all available classes from JARs
        def availableClasses = [] as Set

        jarFiles.each { jarFile ->
            try {
                def zipFile = new java.util.zip.ZipFile(jarFile)
                zipFile.entries().each { entry ->
                    if (entry.name.endsWith(".class") && !entry.name.contains("module-info")) {
                        def className = entry.name
                                .replaceAll(/\.class$/, "")
                                .replace("/", ".")
                        availableClasses << className
                    }
                }
                zipFile.close()
            } catch (Exception e) {
                logger.warn("Could not scan ${jarFile.name}: ${e.message}")
            }
        }

        // Scan for missing dependencies
        logger.lifecycle("\nChecking for missing dependencies")
        def missingDeps = [:].withDefault { [] as Set }

        // JDK packages safe to ignore
        def jdkPackagePrefixes = [
            "java.", "javax.", "jdk.", "sun.", "com.sun.", "org.w3c.", "org.xml.",
            "org.ietf.", "netscape."
        ]

        jarFiles.each { jarFile ->
            try {
                def zipFile = new java.util.zip.ZipFile(jarFile)
                zipFile.entries().each { entry ->
                    if (entry.name.endsWith(".class") && !entry.name.contains("module-info")) {
                        def classBytes = zipFile.getInputStream(entry).bytes
                        def referencedClasses = extractClassReferences(classBytes)

                        referencedClasses.each { refClass ->
                            // Skip JDK classes
                            if (jdkPackagePrefixes.any { refClass.startsWith(it) }) {
                                return
                            }

                            // Skip if already available
                            if (availableClasses.contains(refClass)) {
                                return
                            }

                            // Skip provided packages (available from Logstash at runtime)
                            if (ext.providedPackages.any { refClass.startsWith(it) }) {
                                return
                            }

                            // Skip optional packages
                            if (ext.optionalPackages.any { refClass.startsWith(it) }) {
                                return
                            }

                            // This is a missing dependency!
                            missingDeps[jarFile.name] << refClass
                        }
                    }
                }
                zipFile.close()
            } catch (Exception e) {
                logger.warn("Could not analyze ${jarFile.name}: ${e.message}")
            }
        }

        // Generate Report
        def missingDepsReport = file("${reportDir}/missing-dependencies.txt")

        def allMissingClasses = missingDeps.values().flatten() as Set

        // Group missing by package
        def missingByPackage = allMissingClasses.groupBy { cls ->
            def parts = cls.split('\\.')
            parts.take(Math.min(3, parts.length - 1)).join('.')
        }

        missingDepsReport.withWriter { w ->
            if (missingDeps.isEmpty()) {
                w << "No missing dependencies found.\n"
            } else {
                missingByPackage.sort().each { pkg, classes ->
                    w << "${pkg} (${classes.size()} classes):\n"
                    classes.sort().each { cls ->
                        w << "  -> ${cls}\n"
                    }
                    w << "\n"
                }
            }
        }

        // Fail build if missing deps found
        if (!missingDeps.isEmpty() && ext.failOnMissingDeps) {
            throw new GradleException(
                    "JAR verification failed:\n" +
                    "  - ${allMissingClasses.size()} classes missing from ${missingByPackage.size()} packages\n" +
                    "\nMissing packages: ${missingByPackage.keySet().sort().take(5).join(', ')}" +
                    (missingByPackage.size() > 5 ? "..." : "") +
                    "\n\nReview: ${missingDepsReport.absolutePath}"
            )
        }

        logger.lifecycle("JAR verification finished.")
    }
}

/**
 * Extracts class references from a Java class file by parsing its constant pool.
 */
def extractClassReferences(byte[] classBytes) {
    def references = [] as Set
    try {
        def dis = new DataInputStream(new ByteArrayInputStream(classBytes))
        dis.skipBytes(8) // magic + version
        int cpCount = dis.readUnsignedShort()

        def utf8Strings = [:]
        def classIndices = []

        for (int i = 1; i < cpCount; i++) {
            int tag = dis.readUnsignedByte()
            switch (tag) {
                case 1: // UTF8
                    int len = dis.readUnsignedShort()
                    byte[] bytes = new byte[len]
                    dis.readFully(bytes)
                    utf8Strings[i] = new String(bytes, "UTF-8")
                    break
                case 7: // Class
                    classIndices << dis.readUnsignedShort()
                    break
                case 8: case 16: case 19: case 20: // String, MethodType, Module, Package
                    dis.skipBytes(2)
                    break
                case 3: case 4: // Integer, Float
                    dis.skipBytes(4)
                    break
                case 5: case 6: // Long, Double (take 2 slots)
                    dis.skipBytes(8)
                    i++
                    break
                case 9: case 10: case 11: case 12: case 17: case 18: // Refs, NameAndType, Dynamic, InvokeDynamic
                    dis.skipBytes(4)
                    break
                case 15: // MethodHandle
                    dis.skipBytes(3)
                    break
            }
        }

        classIndices.each { idx ->
            def name = utf8Strings[idx]
            if (name && !name.startsWith("[")) {
                def className = name.replace("/", ".")
                if (className.contains(".")) {
                    references << className
                }
            }
        }
    } catch (Exception e) {
        // Skip malformed class files
    }
    return references
}

compileJava.dependsOn(importMinimalElasticsearch)
shadowJar.dependsOn(importMinimalElasticsearch)

tasks.withType(JavaCompile) {
    options.encoding = 'UTF-8'
}

test {
    useJUnitPlatform()
    testLogging {
        exceptionFormat "full"
        outputs.upToDateWhen { false }
        showStandardStreams = true
    }
}

task generateTestCertificates(type: Exec) {
    def sslTestCertsDir = "${projectDir}/src/test/resources/co/elastic/logstash/filters/elasticintegration/ssl-test-certs"

    workingDir sslTestCertsDir
    commandLine './generate.sh'

    def commonOutputFile = Paths.get("${buildDir}/generateTestCertificates.log")
    file(commonOutputFile.parent).mkdirs()
    def commonOutputStream = Files.newOutputStream(commonOutputFile)

    standardOutput = commonOutputStream
    errorOutput = commonOutputStream

    inputs.file("${sslTestCertsDir}/generate.sh")
    inputs.file("${sslTestCertsDir}/openssl.cnf")

    outputs.dir("${sslTestCertsDir}/generated")
    outputs.file(commonOutputFile)
}
processTestResources.dependsOn(generateTestCertificates)

task geoipTestResources {
    def output = "${project.rootDir}/src/test/resources/co/elastic/logstash/filters/elasticintegration/geoip/databases"

    outputs.dir output
    inputs.files configurations.geolite2

    doLast {
        copy {
            from(zipTree(configurations.geolite2.singleFile)) {
                include "*.mmdb"
            }
            into(output)
        }
    }
}
processTestResources.dependsOn(geoipTestResources)

tasks.withType(Test) {
    // Add Exports to enable tests to run in JDK17
    jvmArgs = [
            "--add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED",
            "--add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED",
            "--add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED",
            "--add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED",
            "--add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED",
            "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED",
            "--add-opens=java.base/java.io=ALL-UNNAMED",
            "--add-opens=java.base/java.lang=ALL-UNNAMED",
            "--add-opens=java.base/java.util=ALL-UNNAMED",
            "-Djdk.attach.allowAttachSelf=true",
            "-Dmockito.mock.maker=inline",
            "-javaagent:${configurations.mockitoAgent.asPath}",
            "-XX:+EnableDynamicAgentLoading"
    ]
}

task vendorShadowJar(dependsOn: shadowJar) {
    description "vendors jar dependencies"

    String vendorPathPrefix = "vendor/jar-dependencies/co/elastic"
    File projectJarFile = file("${vendorPathPrefix}/${project.name}/${project.version}/${project.name}-${project.version}.jar")
    File shadowJar = project.file("${buildDir}/libs/${project.name}-${project.version}-all.jar")

    inputs.file shadowJar
    outputs.dir vendorPathPrefix

    doLast {
        projectJarFile.mkdirs()
        Files.copy(shadowJar.toPath(), projectJarFile.toPath(), REPLACE_EXISTING)
    }
}

task generateGemJarRequiresFile() {
    description "Generates a ruby script for runtime-requiring vendored jars"
    dependsOn vendorShadowJar

    def jarRequiresFile = file("${projectDir}/lib/logstash/filters/elastic_integration/jar_dependencies.rb")

    outputs.file jarRequiresFile
    inputs.property 'project.group', project.group
    inputs.property 'project.name', project.name
    inputs.property 'project.version', project.version

    doLast {
        jarRequiresFile.parentFile.mkdirs()
        jarRequiresFile.withWriter { w ->
            w << "# AUTOGENERATED BY THE GRADLE SCRIPT. DO NOT EDIT.\n\n"
            w << '########################################################################\n' +
                 '# Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V.\n' +
                 '# under one or more contributor license agreements. Licensed under the\n' +
                 '# Elastic License 2.0; you may not use this file except in compliance\n' +
                 '# with the Elastic License 2.0.\n' +
                 '########################################################################\n'
            w << '\n'
            w << "require \'jar_dependencies\'\n"
            w << "require_jar('co/elastic', \'${project.name}\', \'${project.version}\')\n"
        }
    }
}

task generateGemVersionFile() {
    description "Generates a ruby script including version information"

    def versionFile = file("${projectDir}/lib/logstash/filters/elastic_integration/version.rb")
    outputs.file versionFile
    inputs.property 'project.version', project.version

    doLast {
        versionFile.parentFile.mkdirs();
        versionFile.withWriter { w ->
            w << "# AUTOGENERATED BY THE GRADLE SCRIPT. DO NOT EDIT.\n\n"
            w << '########################################################################\n' +
                 '# Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V.\n' +
                 '# under one or more contributor license agreements. Licensed under the\n' +
                 '# Elastic License 2.0; you may not use this file except in compliance\n' +
                 '# with the Elastic License 2.0.\n' +
                 '########################################################################\n'
            w << '\n'
            w << "LogStash::Filters::ElasticIntegration::VERSION='${project.version}'\n"
        }
    }
}

task rubyAssemble {
    description "vendors dependencies and generates required ruby code"

    dependsOn vendorShadowJar
    dependsOn generateGemJarRequiresFile
    dependsOn verifyImportedJars
    dependsOn generateGemVersionFile
}

task vendor {
    description "vendors dependencies and generates the ruby code to require them"
    dependsOn rubyAssemble
}

task publish(dependsOn: vendor) {
    doLast {
        exec {
            commandLine "bundle", "exec", "rake", "publish_gem"
        }
    }
}

task localGem(dependsOn: vendor) {
    doLast {
        exec {
            commandLine "gem", "build", "logstash-filter-elastic_integration.gemspec"
        }
    }
}

task bundleInstall(dependsOn: vendor) {
    def logstashPath = envOrPropertyValue("LOGSTASH_PATH")
    assert logstashPath != null : "The property LOGSTASH_PATH must be defined, and be path to logstash directory"
    inputs.file("logstash-filter-elastic_integration.gemspec")
    inputs.file("Gemfile")
    inputs.property("LOGSTASH_PATH", logstashPath)

    outputs.file("Gemfile.lock")

    doLast {
        exec {
            environment(Map.of("LOGSTASH_PATH", logstashPath, "LOGSTASH_SOURCE", "1"))
            commandLine "bundle", "install"
        }
    }
}

task unitSpecs(dependsOn: bundleInstall) {

    def unitSpecFiles = project.fileTree("spec/unit") { include "**/*_spec.rb" }.files*.path
    assert unitSpecFiles : "No unit spec files could be found"

    inputs.files unitSpecFiles
    inputs.files project.fileTree("lib/**/*.rb")
    inputs.files tasks.named('compileJava').map { it.outputs.files }

    doLast {
        exec {
            commandLine "bundle", "exec", "rspec"
            args "--format=documentation"
            if (project.gradle.startParameter.consoleOutput != ConsoleOutput.Plain) {
                args "--color"
                args "--tty"
            }

            args unitSpecFiles
        }
    }
}

clean {
    delete "${projectDir}/*.gem"
}